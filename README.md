# Reinforcement Learning with PPO: CartPole and Acrobot

## 專案概述
本專案透過強化學習（Reinforcement Learning）中的 **Proximal Policy Optimization (PPO)** 演算法，訓練 Agent 解決兩個經典的 Gymnasium 控制環境：**CartPole-v1** 與 **Acrobot-v1**。專案重點在於觀察 Agent 的學習行為、效能表現，並透過視覺化工具與訓練紀錄進行比較分析。
 

## 環境介紹
### 1. CartPole-v1
* **任務**：將倒立的桿子保持平衡於移動平台上。
* **動作空間**：離散（向左、向右）。
* **終止條件**：桿子傾斜角度過大或超出邊界。

### 2. Acrobot-v1
* **任務**：透過兩節懸臂擺動使其超過特定高度。
* **動作空間**：離散（-1, 0, +1 扭力）。
* **終止條件**：懸臂末端超過指定高度。

## 演算法簡介：PPO
PPO 是一種 on-policy 強化學習演算法，透過限制策略更新幅度（Clip Function）來確保訓練穩定性。本專案使用 **Stable-Baselines3** 提供的實作，並採用 **MlpPolicy** 作為策略網路。

## 訓練設定
* **總訓練步數**：100,000 steps。
* **評估方式**：使用 `evaluate_policy` 計算平均報酬與標準差，並產出測試影片。
* **日誌記錄**：使用 TensorBoard 紀錄 rollout/ep_rew_mean、loss 等指標。

## 實驗結果與分析
### 訓練過程比較
* **CartPole**：訓練極為迅速，約在 20,000 步時達到成長拐點，隨後穩定維持在接近滿分（500分）的表現。
* **Acrobot**：由於任務難度較高且具備欠驅動特性，reward 在 50,000 步後才明顯上升，最終穩定在 -100 至 -150 區間。

### 延伸實驗結論
1. **訓練步數**：較簡單的環境（CartPole）過度訓練可能導致不穩定；較困難的環境（Acrobot）則需 20 萬步以上才能達到理論最佳值。
2. **網路架構**：2層x64個神經元的 Baseline 架構表現最穩；深層網路雖收斂快但波動大，淺層網路則能力不足。
3. **超參數影響**：學習率過大會導致策略震盪甚至崩壞，最佳組合多出現在預設值（學習率 2.5e-4, clip 0.2）附近。

## 結論
透過本次實驗，深入理解了環境複雜度對學習難度的影響。PPO 在解決具備延遲回饋與複雜動力學的控制問題上展現了良好的泛用性與穩定性。
 
